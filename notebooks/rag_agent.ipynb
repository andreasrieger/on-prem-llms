{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3bef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3.2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50bd74c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "vector_size = len(embeddings.embed_query(\"test\"))\n",
    "\n",
    "if not client.collection_exists(\"documents\"):\n",
    "    client.create_collection(\n",
    "        collection_name=\"documents\",\n",
    "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8508be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. /Users/andreas/Code/on-prem-llms/input/20200101-NeurIPS-Language_Models_are_few_shot_learners.pdf\n",
      "2. /Users/andreas/Code/on-prem-llms/input/20171205-Attention_Is_All_You_Need.pdf\n"
     ]
    }
   ],
   "source": [
    "from mypackage import docroot, filemanager, userinput\n",
    "\n",
    "# Get input directory from user\n",
    "_input_dir = docroot.get_input_dir()\n",
    "\n",
    "# List files in the input directory\n",
    "file_list = filemanager.get_files_in_directory(_input_dir, extensions=[\".pdf\"])\n",
    "filemanager.print_enumerated_file_list(file_list)\n",
    "\n",
    "# Get selected file from user\n",
    "selected_file_num = userinput.get_user_input(\"Select a file by number\", default=\"1\")\n",
    "selected_file_path = file_list[int(selected_file_num) - 1]\n",
    "\n",
    "# Process document file\n",
    "file_content_list = filemanager.read_pdf_file(selected_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25528cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mypackage import chunking\n",
    "\n",
    "df = pd.DataFrame(file_content_list, columns=['page_number', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e864c88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25 entries, 0 to 24\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   page_number  25 non-null     int64 \n",
      " 1   text         25 non-null     object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 532.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Language Models are Few-Shot Learners\\nTom B. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Figure 1.1: Performance on SuperGLUE increases...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2 Approach\\nOur basic pre-training approach, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Setting\\nLAMBADA\\n(acc)\\nLAMBADA\\n(ppl)\\nStory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Setting NaturalQS WebQS TriviaQA\\nRAG (Fine-tu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                               text\n",
       "0            1  Language Models are Few-Shot Learners\\nTom B. ...\n",
       "1            2  Figure 1.1: Performance on SuperGLUE increases...\n",
       "2            3  2 Approach\\nOur basic pre-training approach, i...\n",
       "3            4  Setting\\nLAMBADA\\n(acc)\\nLAMBADA\\n(ppl)\\nStory...\n",
       "4            5  Setting NaturalQS WebQS TriviaQA\\nRAG (Fine-tu..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad3cc586",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_content = list(zip(df['page_number'], df['text'].apply(lambda x: chunking.text_chunking(x, chunk_size=512))))\n",
    "df_chunks = pd.DataFrame(chunked_content, columns=['page_number', 'chunks'])\n",
    "df_chunks = df_chunks.explode('chunks').reset_index(drop=True)\n",
    "df_chunks.loc[:, 'chunks'] = df_chunks.loc[:, 'chunks'].apply(lambda x: x.lower())\n",
    "# df_chunks['chunks_lower'] = df_chunks.loc[:, 'chunks'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50df1744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 193 entries, 0 to 192\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   page_number  193 non-null    int64 \n",
      " 1   chunks       193 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>language models are few-shot learners\\ntom b. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>abstract\\nwe demonstrate that scaling up langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>and few-shot demonstrations speciﬁed purely vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>to using task-agnostic pre-training and task-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>agnostic model to perform a desired task.\\nrec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                             chunks\n",
       "0            1  language models are few-shot learners\\ntom b. ...\n",
       "1            1  abstract\\nwe demonstrate that scaling up langu...\n",
       "2            1  and few-shot demonstrations speciﬁed purely vi...\n",
       "3            1  to using task-agnostic pre-training and task-a...\n",
       "4            1  agnostic model to perform a desired task.\\nrec..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks.info()\n",
    "df_chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bee7857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from sqlalchemy import String, create_engine\n",
    "from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\n",
    "\n",
    "class Base(DeclarativeBase):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f058d42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from sqlalchemy import MetaData, Table, Column, Integer, String, create_engine\n",
    "from mypackage import docroot\n",
    "\n",
    "_data_dir = docroot.get_data_dir() + os.sep\n",
    "db_name = 'documents.db'\n",
    "db_path = f\"{_data_dir}{db_name}\"\n",
    "\n",
    "engine = create_engine(f\"sqlite:////{db_path}\")\n",
    "\n",
    "documents_table = Table(\n",
    "    'documents',\n",
    "    Base.metadata,\n",
    "    Column('id', Integer, primary_key=True),\n",
    "    Column('page_number', Integer),\n",
    "    Column('text', String),\n",
    ")\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "df_chunks.to_sql('documents', con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d1870fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns nothing\n",
    "from sqlalchemy import text\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"SELECT * FROM documents\")).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eccf4d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 'language models are few-shot learners\\ntom b. brown∗ benjamin mann∗ nick ryder∗ melanie subbiah∗\\njared kaplan† prafulla dhariwal arvind neelakantan  ... (210 characters truncated) ... ric sigler mateusz litwin scott gray\\nbenjamin chess jack clark christopher berner\\nsam mccandlish alec radford ilya sutskever dario amodei\\nabstract')\n",
      "(1, 1, 'abstract\\nwe demonstrate that scaling up language models greatly improves task-agnostic,\\nfew-shot performance, sometimes even becoming competitive w ... (188 characters truncated) ... uage model, and test its performance in the few-shot setting. for all\\ntasks, gpt-3 is applied without any gradient updates or ﬁne-tuning, with tasks')\n",
      "(2, 1, 'and few-shot demonstrations speciﬁed purely via text interaction with the model.\\ngpt-3 achieves strong performance on many nlp datasets, including t ... (207 characters truncated) ... o training on large web corpora.\\n1 introduction\\nnlp has shifted from learning task-speciﬁc representations and designing task-speciﬁc architectures')\n",
      "(3, 1, 'to using task-agnostic pre-training and task-agnostic architectures. this shift has led to substantial\\nprogress on many challenging nlp tasks such a ... (147 characters truncated) ... sk-\\nagnostic, a ﬁnal task-speciﬁc step remains: ﬁne-tuning on a large dataset of examples to adapt a task\\nagnostic model to perform a desired task.')\n",
      "(4, 1, 'agnostic model to perform a desired task.\\nrecent work [rwc+19] suggested this ﬁnal step may not be necessary. [rwc+19] demonstrated\\nthat a single p ... (83 characters truncated) ... \\n∗equal contribution\\n†johns hopkins university, openai\\n34th conference on neural information processing systems (neurips 2020), vancouver, canada.')\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import select\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "\n",
    "metadata = MetaData()\n",
    "table = Table(\n",
    "    'documents',\n",
    "    metadata,\n",
    "    autoload_with=engine\n",
    "    )\n",
    "\n",
    "stmt = select(table).where(table.columns.page_number == 1)\n",
    "\n",
    "connection = engine.connect()\n",
    "results = connection.execute(stmt).fetchall()\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea608e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"documents\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3f4969c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0706f8f9ce7f4518bcde1df68192a8c6',\n",
       " '899dca2faba94f7786fc0c7f711790f3',\n",
       " 'd0ca9caf48ae4a7bb5a49696b18ba7e2',\n",
       " '6c5ef4200c374e3aabfe3e05aecf5a64',\n",
       " 'd0cc672cbaab4c1f8b0cb7dc48c62f23',\n",
       " 'dd23c0dfcbe145b8b42c3429b0d9ae99']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit = 5\n",
    "texts = df_chunks['chunks'].loc[:limit].tolist()\n",
    "metadatas = [{'page_number': pn} for pn in df_chunks['page_number']]\n",
    "\n",
    "vector_store.add_texts(texts, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e72d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# Construct a tool for retrieving context\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(llm, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce43a0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Welche Informationen enthält das Dokument?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (89bd9939-11c2-4a2f-a624-3b15122771ed)\n",
      " Call ID: 89bd9939-11c2-4a2f-a624-3b15122771ed\n",
      "  Args:\n",
      "    query: Inhalt eines Dokuments\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'page_number': 1, '_id': 'd0ca9caf48ae4a7bb5a49696b18ba7e2', '_collection_name': 'documents'}\n",
      "Content: and few-shot demonstrations speciﬁed purely via text interaction with the model.\n",
      "gpt-3 achieves strong performance on many nlp datasets, including translation,\n",
      "question-answering, and cloze tasks. we also identify some datasets where gpt-\n",
      "3’s few-shot learning still struggles, as well as some datasets where gpt-3 faces\n",
      "methodological issues related to training on large web corpora.\n",
      "1 introduction\n",
      "nlp has shifted from learning task-speciﬁc representations and designing task-speciﬁc architectures\n",
      "\n",
      "Source: {'page_number': 1, '_id': '6c5ef4200c374e3aabfe3e05aecf5a64', '_collection_name': 'documents'}\n",
      "Content: to using task-agnostic pre-training and task-agnostic architectures. this shift has led to substantial\n",
      "progress on many challenging nlp tasks such as reading comprehension, question answering, textual\n",
      "entailment, among others. even though the architecture and initial representations are now task-\n",
      "agnostic, a ﬁnal task-speciﬁc step remains: ﬁne-tuning on a large dataset of examples to adapt a task\n",
      "agnostic model to perform a desired task.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Das Dokument enthält Informationen über das GPT-3-Modell und seine Leistungen in der Naturlichlichen Sprachverarbeitung (NLP). Insbesondere beschreibt es die Stärken und Schwächen des Modells bei verschiedenen NLP-Tasks wie Übersetzung, Fragenbeantworten und Fehlerrückgabetests. Darüber hinaus wird erwähnt, dass das GPT-3-Modell durch eine erste Schrittstufe (\"pre-training\") und eine zweite Anpassungsschritt (\"fine-tuning\") gekennzeichnet ist, um es auf bestimmte Aufgaben anzupassen.\n"
     ]
    }
   ],
   "source": [
    "user_query = userinput.get_user_input(\"Stelle deine Frage!\", default=\"Welche Informationen enthält das Dokument?\")\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
