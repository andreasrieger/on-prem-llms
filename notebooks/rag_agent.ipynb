{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reading data from database",
   "id": "6c7171e9e78f4eee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T06:57:25.401419Z",
     "start_time": "2026-01-19T06:57:25.338879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from mypackage import finder, userinput\n",
    "from sqlalchemy import create_engine, MetaData, Table, select\n",
    "\n",
    "def get_chunks_from_db():\n",
    "\n",
    "    data_dir = os.path.join(finder.get_git_root(), \"data\")\n",
    "    db_name = 'documents.db'\n",
    "    db_path = os.path.join(data_dir, db_name)\n",
    "\n",
    "    engine = create_engine(f\"sqlite:////{db_path}\")\n",
    "\n",
    "    metadata = MetaData()\n",
    "\n",
    "    table = Table(\n",
    "        'documents',\n",
    "        metadata,\n",
    "        autoload_with=engine\n",
    "    )\n",
    "\n",
    "    stmt = select(table)\n",
    "    connection = engine.connect()\n",
    "\n",
    "    results = connection.execute(stmt).fetchall()\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "get_chunks_from_db()"
   ],
   "id": "a5e8fc50c680db68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 'language models are few-shot learners\\ntom b. brown∗ benjamin mann∗ nick ryder∗ melanie subbiah∗\\njared kaplan† prafulla dhariwal arvind neelakantan  ... (210 characters truncated) ... ric sigler mateusz litwin scott gray\\nbenjamin chess jack clark christopher berner\\nsam mccandlish alec radford ilya sutskever dario amodei\\nabstract')\n",
      "(1, 1, 'abstract\\nwe demonstrate that scaling up language models greatly improves task-agnostic,\\nfew-shot performance, sometimes even becoming competitive w ... (188 characters truncated) ... uage model, and test its performance in the few-shot setting. for all\\ntasks, gpt-3 is applied without any gradient updates or ﬁne-tuning, with tasks')\n",
      "(2, 1, 'and few-shot demonstrations speciﬁed purely via text interaction with the model.\\ngpt-3 achieves strong performance on many nlp datasets, including t ... (207 characters truncated) ... o training on large web corpora.\\n1 introduction\\nnlp has shifted from learning task-speciﬁc representations and designing task-speciﬁc architectures')\n",
      "(3, 1, 'to using task-agnostic pre-training and task-agnostic architectures. this shift has led to substantial\\nprogress on many challenging nlp tasks such a ... (147 characters truncated) ... sk-\\nagnostic, a ﬁnal task-speciﬁc step remains: ﬁne-tuning on a large dataset of examples to adapt a task\\nagnostic model to perform a desired task.')\n",
      "(4, 1, 'agnostic model to perform a desired task.\\nrecent work [rwc+19] suggested this ﬁnal step may not be necessary. [rwc+19] demonstrated\\nthat a single p ... (83 characters truncated) ... \\n∗equal contribution\\n†johns hopkins university, openai\\n34th conference on neural information processing systems (neurips 2020), vancouver, canada.')\n",
      "(5, 2, 'figure 1.1: performance on superglue increases with model size. a value ofk = 32 means\\nthat our model was shown 32 examples per task, for 256 exampl ... (175 characters truncated) ... et results are in the appendix). the bert-large reference\\nmodel was ﬁne-tuned on the superglue training set (125k examples), whereas bert++ was ﬁrst')\n",
      "(6, 2, 'ﬁne-tuned on multinli (392k examples) and sw ag (113k examples) before further ﬁne-tuning on\\nthe superglue training set (for a total of 630k ﬁne-tun ... (133 characters truncated) ... n the bert-large and bert++ to be roughly equivalent to the difference\\nbetween gpt-3 with one example per context versus eight examples per context.')\n",
      "(7, 2, 'aggregate performance for all 42 accuracy-denominated benchmarks. while zero-shot perfor-\\nmance improves steadily with model size, few-shot performa ... (152 characters truncated) ... of training examples. while this work was a promising\\nproof of concept, the best case performance only matched some supervised baselines on a single')\n",
      "(8, 2, 'dataset. on most tasks, performance was still far from even simple supervised baselines.\\nhowever [rwc+19] also showed a potential way forward. the w ... (181 characters truncated) ... nducted a much more rigorous study of the scaling\\nbehavior of log loss and conﬁrmed smooth scaling trends. in this work, we empirically test whether')\n",
      "(9, 2, 'scaling continues to improve performance by extrapolating the previously identiﬁed phenomena\\nanother two orders of magnitude. we train a 175 billion ... (157 characters truncated) ... and systematize the approach introduced in [rwc+19].\\nwhile [rwc+19] describe their work as “zero-shot task transfer” they sometimes provide examples')\n",
      "(10, 2, 'of the relevant task in the context. due to the use of what are effectively training examples, these\\ncases are better described as “one-shot” or “fe ... (200 characters truncated) ... formed. our ﬁndings are summarized in figure 1.1. we\\nobserve that one- and few-shot performance is often much higher than true zero-shot performance')\n",
      "(11, 2, 'leading us to suggest that language models can also be understood as meta-learners where slow\\nouter-loop gradient descent based learning is combined ... (140 characters truncated) ... esults in the zero- and one-shot settings, and in\\nthe few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art')\n",
      "(12, 2, '(despite state-of-the-art being held by ﬁne-tuned models). for example, gpt-3 achieves 81.5 f1 on\\ncoqa in the zero-shot setting, 84.0 f1 on coqa in  ... (167 characters truncated) ... ng, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to\\nﬁne-tuned models operating in the same closed-book setting.')\n",
      "(13, 2, 'ﬁne-tuned models operating in the same closed-book setting.\\nwe additionally train a series of smaller models (ranging from 125 million parameters to ... (168 characters truncated) ...  tasks with model capacity in all three settings;\\none notable pattern is that the gap between zero-, one-, and few-shot performance often grows with')\n",
      "(14, 2, 'model capacity, perhaps suggesting that larger models are more proﬁcient meta-learners.\\n2')\n",
      "(15, 3, '2 approach\\nour basic pre-training approach, including model, data, and training, is similar to the process\\ndescribed in [rwc+19], with relatively s ... (197 characters truncated) ... lore different settings for learning within the context:\\n• fine-tuning (ft) - updates the weights of a pre-trained model by training on thousands of')\n",
      "(16, 3, 'supervised labels speciﬁc to the desired task. the main advantage of ﬁne-tuning is strong\\nperformance on many benchmarks. the main disadvantages are ... (129 characters truncated) ... tential to exploit spurious features of the training data [gsl+18, nk19]. we focus\\non task-agnostic performance, leaving ﬁne-tuning for future work.')\n",
      "(17, 3, '• few-shot (fs) - the model is given a few demonstrations of the task at inference time as\\nconditioning [rwc+19], but no weights are updated. an exa ... (141 characters truncated) ... ivingk examples of context and completion, and then one ﬁnal\\nexample of context, with the model expected to provide the completion (see appendix for')\n",
      "(18, 3, 'more details). we typically setk in the range of 10 to 100, as this is how many examples can\\nﬁt in the model’s context window (nctx = 2048). the mai ... (164 characters truncated) ...  than state-of-the-art ﬁne-tuned models. also, a small\\namount of task speciﬁc data is still required. as indicated by the name, few-shot learning as')\n",
      "(19, 3, 'described here for language models is related to few-shot learning as used in other contexts\\nin ml [hyc01, vbl+16] – both involve learning based on  ... (194 characters truncated) ... description of the task\\ninstead of any examples.\\nthe appendix includes a demonstration of the four methods using the example of translating english')\n",
      "(20, 3, 'to french. while the few-shot results we present in this paper achieve the highest performance,\\none-shot, or even sometimes zero-shot, seem like the ... (161 characters truncated) ... t-2 [rwc+19], including the modiﬁed initialization,\\npre-normalization, and reversible tokenization described therein, with the exception that we use')\n",
      "(21, 3, 'alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar\\nto the sparse transformer [cgrs19]. to stud ... (125 characters truncated) ... llion parameters, with the\\nlast being the model we call gpt-3. this range of model sizes allows us to test the scaling laws\\nintroduced in [kmh+20].')\n",
      "(22, 3, 'introduced in [kmh+20].\\nmore details on the sizes and architectures of our models can be found in the appendix. we partition\\neach model across gpus ... (164 characters truncated) ... nd ﬁltered a version of commoncrawl1 [rsr+19]\\nbased on similarity to a range of high-quality reference corpora, (2) performed fuzzy deduplication at')\n",
      "(23, 3, 'the document level, within and across datasets, to prevent redundancy and preserve the integrity of\\nour held-out validation set as an accurate measu ... (130 characters truncated) ... diversity. these\\nreference corpora include an expanded version of the webtext dataset [ rwc+19], collected by\\n1https://commoncrawl.org/the-data/\\n3')\n",
      "(24, 4, 'setting\\nlambada\\n(acc)\\nlambada\\n(ppl)\\nstorycloze\\n(acc)\\nhellaswag\\n(acc)\\nsota 68.0 a 8.63b 91.8c 85.6d\\ngpt-3 zero-shot 76.2 3.00 83.2 78.9\\ngpt ... (145 characters truncated) ... roves sota on\\nlambada while achieving respectable performance on two difﬁcult completion prediction datasets.\\na[tur20] b[rwc+19] c[ldl19] d[lch+20]')\n",
      "(25, 4, 'a[tur20] b[rwc+19] c[ldl19] d[lch+20]\\nscraping links over a longer period of time, and ﬁrst described in [kmh+20], two internet-based\\nbooks corpora ... (145 characters truncated) ...  can typically use a larger batch size, but require\\na smaller learning rate. we measure the gradient noise scale during training and use it to guide')\n",
      "(26, 4, 'our choice of batch size [mkat18]. table a.1 shows the parameter settings we used. to train the\\nlarger models without running out of memory, we use  ... (150 characters truncated) ... pu’s on part of a high-bandwidth cluster. details of the training process and hyperparameter\\nsettings are described in the appendix.\\n2.4 evaluation')\n",
      "(27, 4, 'settings are described in the appendix.\\n2.4 evaluation\\nfor few-shot learning, we evaluate each example in the evaluation set by randomly drawing k\\ ... (122 characters truncated) ... and storycloze there is no supervised training set available so we draw\\nconditioning examples from the development set and evaluate on the test set.')\n",
      "(28, 4, 'for some tasks we use a natural language prompt in addition to (or fork = 0, instead of) demonstra-\\ntions. similar to [rsr+19] we also sometimes cha ... (169 characters truncated) ... beam width of 4 and a length penalty ofα = 0.6.\\nfinal results are reported on the test set when publicly available, for each model size and learning')\n",
      "(29, 4, 'setting (zero-, one-, and few-shot). when the test set is private, our model is often too large to ﬁt on\\nthe test server, so we report results on th ... (218 characters truncated) ... n the penn tree bank (ptb) [ mkm+94] dataset measured in\\n[rwc+19]. we omit the 4 wikipedia-related tasks and the one-billion word benchmark due to a')\n",
      "(30, 4, 'high fraction of these datasets being contained in our training set. our largest model sets a new sota\\non ptb by a substantial margin of 15 points.\\ ... (136 characters truncated) ...  models is yielding diminishing returns on this\\nbenchmark, we ﬁnd that zero-shot gpt-3 achieves a substantive gain of 8% over the previous state-of-')\n",
      "(31, 4, 'the-art. for the few-shot setting, we use a ﬁll-in-the-blank format to encourage the language model to\\nonly generate one word (alice was friends with bob. alice went to visit her friend, .→ bob).\\nwith this format, gpt-3 achieves an increase of over 18% from the previous state-of-the-art, and\\n4')\n",
      "(32, 5, 'setting naturalqs webqs triviaqa\\nrag (fine-tuned, open-domain) [lpp+20] 44.5 45.5 68.0\\nt5-11b+ssm (fine-tuned, closed-book) [rrs20] 36.6 44.7 60.5\\ ... (179 characters truncated) ... main qa tasks. gpt-3 is shown in the few-, one-, and\\nzero-shot settings, as compared to prior sota results for closed book and open domain settings.')\n",
      "(33, 5, 'triviaqa few-shot result is evaluated on the wiki split test server.\\nsetting arc (easy) arc (challenge) coqa drop\\nfine-tuned sota 92.0a 78.5b 90.7c ... (159 characters truncated) ...  qa / rc tasks. coqa and drop are f1 while arc\\nreports accuracy. see the appendix for additional experiments. a[kks+20] b[kks+20] c[jzc+19]\\nd[jn20]')\n",
      "(34, 5, 'd[jn20]\\nperformance improves smoothly with model size. however, the ﬁll-in-blank method is not effective\\none-shot, where it always performs worse t ... (184 characters truncated) ...  the lambada dataset appears to be present in our training data – however\\nanalysis performed in section 4 suggests negligible impact on performance.')\n",
      "(35, 5, 'the hellaswag dataset [zhb+19] involves picking the best ending to a story or set of instructions.\\nthe examples were adversarially mined to be difﬁc ... (177 characters truncated) ...  sota achieved by the ﬁne-tuned multi-task model alum.\\nthe storycloze 2016 dataset [ mch+16] involves selecting the correct ending sentence for ﬁve-')\n",
      "(36, 5, 'sentence long stories. here gpt-3 improves over previous zero-shot results by roughly 10% but is\\noverall still 4.1% lower than the ﬁne-tuned sota us ... (206 characters truncated) ...  about broad factual knowledge. we evaluate in the\\n“closed-book” setting (meaning no conditioning information/articles) as suggested by [ rrs20]. on')\n",
      "(37, 5, 'triviaqa [jcwz17], gpt-3 zero-shot already outperforms the ﬁne-tuned t5-11b by 14.2%, and also\\noutperforms a version with q&a tailored span predicti ... (179 characters truncated) ... etrieval mechanism over a 15.3b parameter dense vector\\nindex of 21m documents [lpp+20]. gpt-3’s few-shot result further improves performance another')\n",
      "(38, 5, '3.2% beyond this. on natural questions (nqs) [kpr+19], gpt-3 underperforms a ﬁne-tuned t5\\n11b+ssm. the questions in nqs tend towards ﬁne-grained wik ... (154 characters truncated) ... set of multiple-choice questions collected from\\n3rd to 9th grade science exams. on the “challenge” version of the dataset, which has been ﬁltered to')\n",
      "(39, 5, 'questions which simple statistical or information retrieval methods are unable to correctly answer,\\ngpt-3 approaches the performance of a ﬁne-tuned roberta baseline [kks+20]. on the “easy”\\n5')\n",
      "(40, 6, 'setting en →fr fr →en en →de de →en en →ro ro →en\\nsota (supervised) 45.6a 35.0 b 41.2c 40.2d 38.5e 39.9e\\nxlm [lc19] 33.4 33.3 26.4 34.3 33.3 31.8\\n ... (153 characters truncated) ... 3.7 26.2 30.4 20.6 38.6\\ngpt-3 few-shot 32.6 39.2 29.7 40.6 21.0 39.5\\ntable 3.4: few-shot gpt-3 outperforms previous unsupervised nmt work by 5 bleu')\n",
      "(41, 6, 'when translating into english reﬂecting its strength as an english lm. we report bleu\\nscores on the wmt’14 fr ↔en, wmt’16 de↔en, and wmt’16 ro ↔en d ... (140 characters truncated) ... leu f [pos18] results reported in the appendix. under-\\nline indicates an unsupervised or few-shot sota, bold indicates supervised sota with relative')\n",
      "(42, 6, 'conﬁdence. a[eoag18] b[dhkh14] c[wxh+18] d[or16] e[lgg+20] f [sacrebleu signature:\\nbleu+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20]\\nve ... (137 characters truncated) ...  worse than the overall sotas achieved by [kks+20].\\nfinally, we evaluate gpt-3 on two reading comprehension datasets. few-shot gpt-3 performs within')\n",
      "(43, 6, '3 points of the human baseline on coqa [rcm19], a free-form conversational dataset. on drop\\n[dwd+19], a dataset testing discrete reasoning and numer ... (191 characters truncated) ... rks with symbolic systems [rll+19].\\n3.3 translation\\nin collecting training data for gpt-3, we used the unﬁltered distribution of languages reﬂected')\n",
      "(44, 6, 'in internet text datasets (primarily common crawl). as a result, although gpt-3’s training data\\nprimarily consists of english (93% by word count), i ... (195 characters truncated) ... anslation [shb15] to bridge the two languages in a\\ncontrolled way. by contrast, gpt-3 learns from a blend of training data that mixes many languages')\n",
      "(45, 6, 'together. additionally, our one / few-shot settings aren’t strictly comparable to prior unsupervised\\nwork since they make use of a small amount of p ... (180 characters truncated) ... performance with prior work. few-shot gpt-3 further\\nimproves another 4 bleu resulting in similar average performance to prior unsupervised nmt work.')\n",
      "(46, 6, 'for the three input languages studied, gpt-3 signiﬁcantly outperforms prior unsupervised nmt work\\nwhen translating into english but underperforms wh ... (193 characters truncated) ... vel bpe tokenizer of gpt-2 which was developed for\\nan almost entirely english training dataset. for both fr-en and de-en, few shot gpt-3 outperforms')\n",
      "(47, 6, 'the best supervised result we could ﬁnd but due to our unfamiliarity with the literature and the\\nappearance that these are un-competitive benchmarks ... (192 characters truncated) ...  examples, and backtranslation [lhcg19b].\\n3.4 superglue\\nthe superglue benchmark is a standardized collection of datasets [ wpn+19]. in the few-shot')\n",
      "(48, 6, 'setting, we used 32 examples for all tasks, sampled randomly from the training set. for all tasks\\nexcept wsc and multirc, we sampled a new set of examples to use in the context for each problem.\\nfor wsc and multirc, we used the same set of randomly drawn examples from the training set\\n6')\n",
      "(49, 7, 'superglue boolq cb cb copa rte\\naverage accuracy accuracy f1 accuracy accuracy\\nfine-tuned sota 89.0 91.0 96.9 93.9 94.8 92.5\\nfine-tuned bert-large  ... (155 characters truncated) ...  f1\\nfine-tuned sota 76.1 93.8 62.3 88.2 92.5 93.3\\nfine-tuned bert-large 69.6 64.6 24.1 70.0 71.3 72.0\\ngpt-3 few-shot 49.4 80.1 30.5 75.4 90.2 91.1')\n",
      "(50, 7, 'gpt-3 few-shot 49.4 80.1 30.5 75.4 90.2 91.1\\ntable 3.5: performance of gpt-3 on superglue compared to ﬁne-tuned baselines and sota. all\\nresults are ... (185 characters truncated) ... ated. we sweep values of k up to 32 and note that the\\nfew-shot superglue score steadily improves with both model size and with number of examples in')\n",
      "(51, 7, 'the context showing increasing beneﬁts from in-context learning (figure 1.1).\\nwe observe a wide range in gpt-3’s performance across tasks. on copa a ... (149 characters truncated) ... cond place on the leaderboard, where ﬁrst place is held by\\na ﬁne-tuned 11 billion parameter model (t5). on wsc, boolq, multirc, and rte, performance')\n",
      "(52, 7, 'is reasonable, roughly matching that of a ﬁne-tuned bert-large. on cb, we see signs of life at\\n75.6% in the few-shot setting. wic is a notable weak  ... (188 characters truncated) ...  same meaning in two sentences), none of which was\\nable to achieve strong performance. this hints at a phenomenon (which we saw in other experiments')\n",
      "(53, 7, 'we ran contained in the additional materials) – gpt-3 appears to be weak in the few-shot or one-shot\\nsetting at some tasks that involve comparing tw ... (178 characters truncated) ...  ﬁne-tuned bert-large on four of eight tasks and on two tasks gpt-3 is\\nclose to the state-of-the-art held by a ﬁne-tuned 11 billion parameter model.')\n",
      "(54, 7, '4 measuring and preventing memorization of benchmarks\\nthe dataset and model size are about two orders of magnitude larger than those used for gpt-2, ... (150 characters truncated) ... e large amount of data, even gpt-3 175b does\\nnot overﬁt its training set by a signiﬁcant amount, measured relative to a held-out validation set with')\n",
      "(55, 7, 'which it was deduplicated. for each benchmark, we produce a ‘clean’ version which removes all\\npotentially leaked examples, deﬁned roughly as example ... (202 characters truncated) ... are to the original score. if the score on\\nthe clean subset is similar to the score on the entire dataset, this suggests that contamination, even if')\n",
      "(56, 7, 'present, does not have a signiﬁcant effect on reported results. in most cases performance changes only\\nnegligibly, and we see no evidence that conta ... (163 characters truncated) ... mination has little effect on performance. we provide full details of the methodology and\\nanalysis on the most problematic tasks in the appendix.\\n7')\n",
      "(57, 8, '5 limitations\\non text synthesis, gpt-3 samples still sometimes repeat themselves semantically at the document\\nlevel, start to lose coherence over s ... (130 characters truncated) ... itory contains uncurated unconditional\\nsamples.\\nour experiments do not include any bidirectional architectures or other training objectives such as')\n",
      "(58, 8, 'denoising. our design decision comes at the cost of potentially worse performance on tasks which\\nempirically beneﬁt from bidirectionality, such as ﬁ ... (182 characters truncated) ...  then generating a very short answer (quac, race).\\nour objective weights every token equally and lacks a notion of what is most important to predict')\n",
      "(59, 8, 'and what is less important. [ rrs20] demonstrate beneﬁts of customizing prediction to entities of\\ninterest. also, with self-supervised objectives, t ... (213 characters truncated) ... ons rather than just making predictions. finally,\\nlarge pretrained language models are not grounded in other domains of experience, such as video or')\n",
      "(60, 8, 'real-world physical interaction, and thus lack a large amount of context about the world [bht+20].\\nfor all these reasons, scaling pure self-supervis ... (202 characters truncated) ...  humans [ zsw+19], ﬁne-tuning with reinforcement\\nlearning, or adding additional modalities such as images to provide grounding and a better model of')\n",
      "(61, 8, 'the world [cly+19].\\ngpt-3’s size makes it challenging to deploy. task-speciﬁc distillation [hvd15] merits exploration at\\nthis new scale.\\n6 related ... (148 characters truncated) ... nd in loss as autoregressive language models are\\nscaled up. there are different approaches to scaling language models through increasing parameters,')\n",
      "(62, 8, 'compute, or both. our work is most aligned with methods that have increased the size of transformers\\nby increasing parameters and flops-per-token ro ... (193 characters truncated) ... ecently 17 billion [tur20]. a second line of work has\\nfocused on increasing parameter count but not computation by using the conditional computation')\n",
      "(63, 8, 'framework [blc13]. speciﬁcally, the mixture-of-experts method [ smm+17] has produced 100\\nbillion parameter models and 50 billion parameter translati ... (173 characters truncated) ... jys+19, kr16] approaches to distillation. lastly, a\\nthird approach to scale increases computation without increasing parameters through methods like')\n",
      "(64, 8, 'adaptive computation time [gra16] and the universal transformer [dgv+18].\\nthere are many approaches to building multi-task models. giving task instr ... (166 characters truncated) ... e-tuning. multi-task learning [car97] has shown some\\npromising initial results [lgh+15, lcr19] and multi-stage ﬁne-tuning has produced sota or sota-')\n",
      "(65, 8, 'competitive results [pfb18, kks+20]. metalearning was used in language models in [rwc+19],\\nthough with limited results and no systematic study. othe ... (173 characters truncated) ... ntext with previous examples is most structurally\\nsimilar to rl2. it also resembles [ hyc01], in that an inner loop adapts to a task, while an outer')\n",
      "(66, 8, 'loop updates the weights. our inner loop performs few-shot in-context learning, but prior work has\\nexplored other methods of few-shot learning [ss20 ... (169 characters truncated) ... dclt18], preﬁxlm [dl15], encoder-decoder architectures\\n[llg+19, rsr+19], random permutations during training [ydy+19], architectures for sampling\\n8')\n",
      "(67, 9, 'efﬁciency [dyy+19], data and training improvements [log+19], and embedding parameters efﬁ-\\nciency [lcg+19]. it is likely that incorporating some of  ... (175 characters truncated) ... nguage model which shows strong performance on many\\nnlp tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly')\n",
      "(68, 9, 'matching the performance of state-of-the-art ﬁne-tuned systems, as well as generating high-quality\\nsamples and strong qualitative performance at tas ... (214 characters truncated) ... es, these results suggest that\\nvery large language models may be an important ingredient in the development of adaptable, general\\nlanguage systems.')\n",
      "(69, 9, 'language systems.\\nfunding disclosures\\nthis work was funded by openai. all models were trained on v100 gpu’s on part of a high-\\nbandwidth cluster p ... (183 characters truncated) ... arrative generation, improving search engine responses,\\nand answering questions. but they also have potentially harmful applications. gpt-3 improves')\n",
      "(70, 9, 'the quality of text generation and adaptability over smaller models and increases the difﬁculty of\\ndistinguishing synthetic text from human-written  ... (163 characters truncated) ... language models, not because we believe the\\nharms are necessarily greater, but in order to stimulate efforts to study and mitigate them. the broader')\n",
      "(71, 9, 'impacts of language models like this are numerous. we focus on two primary issues: the potential\\nfor deliberate misuse of language models like gpt-3 ... (148 characters truncated) ... nefﬁciency (section 7.3).\\n7.1 misuse of language models\\nmalicious uses of language models can be somewhat difﬁcult to anticipate because they often')\n",
      "(72, 9, 'involve repurposing language models in a very different environment or for a different purpose than\\nresearchers intended. to help with this, we can  ... (196 characters truncated) ... ination of likelihood and impact [ros12]. we discuss three\\nfactors: potential misuse applications, threat actors, and external incentive structures.')\n",
      "(73, 9, '7.1.1 potential misuse applications\\nany socially harmful activity that relies on generating text could be augmented by powerful lan-\\nguage models.  ... (128 characters truncated) ... nd social engineering pretexting. many of these\\napplications bottleneck on human beings to write sufﬁciently high quality text. language models that')\n",
      "(74, 9, 'produce high quality text generation could lower existing barriers to carrying out these activities and\\nincrease their efﬁcacy.\\nthe misuse potentia ... (116 characters truncated) ... paragraphs of synthetic content that people ﬁnd difﬁcult to\\ndistinguish from human-written text represents a concerning milestone in this regard.\\n9')\n",
      "(75, 10, '7.1.2 threat actor analysis\\nthreat actors can be organized by skill and resource levels, ranging from low or moderately skilled\\nand resourced actor ... (134 characters truncated) ... -sponsored) groups with long-term agendas\\n[sbc+19].\\nto understand how low and mid-skill actors think about language models, we have been monitoring')\n",
      "(76, 10, 'forums and chat groups where misinformation tactics, malware distribution, and computer fraud\\nare frequently discussed. while we did ﬁnd signiﬁcant  ... (193 characters truncated) ...  discussions were correlated with media coverage\\nof language model technologies. from this, we assess that the threat of misuse from these actors is')\n",
      "(77, 10, 'not immediate, but signiﬁcant improvements in reliability could change this.\\nbecause apts do not typically discuss operations in the open, we have c ... (177 characters truncated) ... e in operations that may see potential gains by using\\nlanguage models. the assessment was that language models may not be worth investing signiﬁcant')\n",
      "(78, 10, 'resources in because there has been no convincing demonstration that current language models are\\nsigniﬁcantly better than current methods for genera ... (217 characters truncated) ...  techniques, and procedures (ttps) that they rely\\non to accomplish their agenda. ttps are inﬂuenced by economic factors like scalability and ease of')\n",
      "(79, 10, 'deployment; phishing is extremely popular among all groups because it offers a low-cost, low-effort,\\nhigh-yield method of deploying malware and stea ... (177 characters truncated) ... ving stable infrastructure has a large impact on the\\nadoption of ttps. the outputs of language models are stochastic, however, and though developers')\n",
      "(80, 10, 'can constrain these (e.g. using top-k truncation) they are not able to perform consistently without\\nhuman feedback. if a social media disinformation ... (135 characters truncated) ... of human labor\\nrequired in operating this bot. but a human is still needed to ﬁlter the outputs, which restricts how\\nscalable the operation can be.')\n",
      "(81, 10, 'scalable the operation can be.\\nbased on our analysis of this model and analysis of threat actors and the landscape, we suspect ai\\nresearchers will  ... (204 characters truncated) ... rch community, and hope to work on this through a combination of mitigation research,\\nprototyping, and coordinating with other technical developers.')\n",
      "(82, 10, 'prototyping, and coordinating with other technical developers.\\n7.2 fairness, bias, and representation\\nbiases present in training data may lead mode ... (192 characters truncated) ... ucing demeaning portrayals amongst other potential\\nharms [cra17]. we have conducted an analysis of biases in the model in order to better understand')\n",
      "(83, 10, 'gpt-3’s limitations when it comes to fairness, bias, and representation. 2\\nour goal is not to exhaustively characterize gpt-3, but to give a prelimi ... (179 characters truncated) ... could be studied in follow-up work. this is a\\npreliminary analysis and does not reﬂect all of the model’s biases even within the studied categories.')\n",
      "(84, 10, 'broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to\\nreﬂect stereotypes present in their training ... (83 characters truncated) ... , and representation in language models is a rapidly-developing area with a large\\nbody of prior work. see, for example, [hzj+19, nbr20, scnp19].\\n10')\n",
      "(85, 11, 'along the dimensions of gender, race, and religion. we probe for bias in the 175 billion parameter\\nmodel and also in similar smaller models, to see  ... (207 characters truncated) ... ave a higher probability of being followed by a\\nmale gender identiﬁer than a female one (in other words, they are male leaning) when given a context')\n",
      "(86, 11, 'such as \"the {occupation} was a\" (neutral variant). 83% of the 388 occupations we tested\\nwere more likely to be followed by a male identiﬁer by gpt- ... (181 characters truncated) ...  male etc.) or female indicating words (woman,\\nfemale etc.). in particular, occupations demonstrating higher levels of education such as legislator,')\n",
      "(87, 11, 'banker, or professor emeritus were heavily male leaning along with occupations that require hard\\nphysical labour such as mason, millwright, and sher ... (159 characters truncated) ... obabilities changed when we shifted the context to be the \"the\\ncompetent {occupation} was a\" (competent variant), and when we shifted the context to')\n",
      "(88, 11, 'be \"the incompetent {occupation} was a\" (incompetent variant) for each occupation in the\\ndataset. we found that, when prompted with \"the competent { ... (163 characters truncated) ... ur original neutral prompt, \"the {occupation} was a\" . with\\nthe prompt \"the incompetent {occupation} was a\" the majority of occupations still leaned')\n",
      "(89, 11, 'male with a similar probability than for our original neutral prompt. the average occupation bias -\\nmeasured as 1\\nnjobs\\n∑\\njobs log( p (female|con ... (153 characters truncated) ...  pronoun resolution on the winogender dataset [ rnlvd18] using\\ntwo methods which further corroborated the model’s tendency to associate most occupa-')\n",
      "(90, 11, 'tions with males. one method measured the models ability to correctly assign a pro-\\nnoun as the occupation or the participant. for example, we fed t ... (154 characters truncated) ... he option with the lowest\\nprobability between the two possible options (choices between occupation option: advisor; partici-\\npant option: advisee).')\n",
      "(91, 11, 'pant option: advisee).\\noccupation and participant words often have societal biases associated with them such as the\\nassumption that most occupants  ... (218 characters truncated) ... cy of all the models (64.17%) on this task. it was\\nalso the only model where the accuracy for occupant sentences (sentences where the correct answer')\n",
      "(92, 11, 'was the occupation option) for females was higher than for males (81.7% vs 76.7%). all other\\nmodels had a higher accuracy for male pronouns with occ ... (182 characters truncated) ... y evidence that in places where issues of bias can make\\nlanguage models susceptible to error, the larger models are more robust than smaller models.')\n",
      "(93, 11, 'we also performed co-occurrence tests, where we analyzed which words are likely to occur in the\\nvicinity of other pre-selected words. we created a m ... (176 characters truncated) ... s \"he was very\" , \"she was very\" , \"he would be described\\nas\", \"she would be described as\" 3. we looked at the adjectives and adverbs in the top 100')\n",
      "(94, 11, 'most favored words using an off-the-shelf pos tagger [lb02]. we found females were more often\\ndescribed using appearance oriented words such as ”bea ... (194 characters truncated) ...  it easier to study co-occurrence\\nsince it does not require the isolation of instances in which ‘they’ refers to a singular noun from those where it')\n",
      "(95, 11, 'didn’t, but other forms of gender bias are likely present and could be studied using different approaches.\\n11')\n",
      "(96, 12, 'table 7.1: most biased descriptive words in 175b model\\ntop 10 most biased male descriptive words\\nwith raw co-occurrence counts\\ntop 10 most biased  ... (210 characters truncated) ... 5) bubbly (12)\\nlazy (14) naughty (12)\\nfantastic (13) easy-going (12)\\neccentric (13) petite (10)\\nprotect (10) tight (10)\\njolly (10) pregnant (10)')\n",
      "(97, 12, 'protect (10) tight (10)\\njolly (10) pregnant (10)\\nstable (9) gorgeous (28)\\npersonable (22) sucked (8)\\nsurvive (7) beautiful (158)\\ntable 7.1 shows ... (126 characters truncated) ... oun indicator. “most favored” here indicates words\\nwhich were most skewed towards a category by co-occurring with it at a higher rate as compared to')\n",
      "(98, 12, 'the other category. to put these numbers in perspective, we have also included the average for the\\nnumber of co-occurrences across all qualifying wo ... (152 characters truncated) ... } woman was very\" and \"people would describe the {race}\\nperson as\" and generated 800 samples for each of the above prompts, with{race} replaced with')\n",
      "(99, 12, 'a term indicating a racial category such as white or asian. we then measure word co-occurrences\\nin the generated samples. given prior research demon ... (179 characters truncated) ... nt using senti wordnet [ bes10] for the words which\\nco-occurred disproportionately with each race. each word sentiment varied from 100 to -100, with')\n",
      "(100, 12, 'positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores\\nindicating negative words (eg. wretched: -87.5 , ... (134 characters truncated) ... g the models to talk about race and this in turn\\ngenerated text that focused on racial features; these results are not from the models talking about')\n",
      "(101, 12, 'race in the wild but talking about race in an experimental setup where they have been primed to do\\nso. additionally, since we are measuring sentimen ... (164 characters truncated) ... ill frequently have a negative sentiment, which may lead to a demographic being associated\\nwith a negative sentiment under this testing methodology.')\n",
      "(102, 12, 'with a negative sentiment under this testing methodology.\\nacross the models we analyzed, ‘asian’ had a consistently high sentiment - it ranked 1st i ... (169 characters truncated) ... the larger model sizes. this analysis gives a\\nsense of the biases of different models and highlights the need for more sophisticated analysis of the')\n",
      "(103, 12, 'relationship between sentiment, entities, and input data.\\n7.2.3 religion\\nwe studied which words co-occurred with religious terms relating to atheis ... (168 characters truncated) ... prompts were of the nature \"{religion practitioners}\\nare\" (eg. \"christians are\" ) for each of the six religious categories listed above. we then\\n12')\n",
      "(104, 13, 'figure 7.1: racial sentiment across models\\nreligion most favored descriptive words\\natheism ‘theists’, ‘cool’, ‘agnostics’, ‘mad’, ‘theism’, ‘defens ... (177 characters truncated) ... ent’, ‘non-violent’\\nchristianity ‘attend’, ‘ignorant’, ‘response’, ‘judgmental’, ‘grace’, ‘execution’, ‘egypt’,\\n‘continue’, ‘comments’, ‘ofﬁcially’')\n",
      "(105, 13, '‘continue’, ‘comments’, ‘ofﬁcially’\\nhinduism ‘caste’, ‘cows’, ‘bjp’, ‘kashmir’, ‘modi’, ‘celebrated’, ‘dharma’, ‘pakistani’,\\n‘originated’, ‘africa’ ... (171 characters truncated) ... blacks’, ‘smartest’, ‘racists’, ‘arabs’,\\n‘game’, ‘russian’\\ntable 7.2: shows the ten most favored words about each religion in the gpt-3 175b model.')\n",
      "(106, 13, 'allowed the model to naturally carry out completions and created a corpus of such completions for\\nstudying co-occurrence of words.\\nthe following is ... (122 characters truncated) ... the more conservative branch, centering on monastic life\\nand the earliest sutras and refusing to recognize the later mahayana\\nsutras as authentic.\"')\n",
      "(107, 13, 'sutras as authentic.\"\\nsimilar to race, we found that the models make associations with religious terms that indicate some\\npropensity to reﬂect how  ... (116 characters truncated) ...  ramadan, prophet and mosque co-occurred at a higher\\nrate than for other religions. we also found that words such asviolent, terrorism and terrorist')\n",
      "(108, 13, 'co-occurred at a greater rate with islam than with other religions and were in the top 40 most favored\\nwords for islam in gpt-3.\\n7.2.4 future bias  ... (176 characters truncated) ... difﬁculties in characterizing biases in large-scale\\ngenerative models; we expect this to be an area of continuous research for us and are excited to')\n",
      "(109, 13, 'discuss different methodological approaches with the community. we view the work in this section\\n13')\n",
      "(110, 14, 'figure 7.2: total compute used during training. based on the analysis in scaling laws for neural\\nlanguage models [kmh+20] we train much larger model ... (124 characters truncated) ... arams),\\nboth models took roughly 50 petaﬂop/s-days of compute during pre-training. methodology for these\\ncalculations can be found in the appendix.')\n",
      "(111, 14, 'calculations can be found in the appendix.\\nas subjective signposting - we chose gender, race, and religion as a starting point, but we recognize\\nth ... (153 characters truncated) ... model cards for model reporting from [mwz+18].\\nultimately, it is important not just to characterize biases in language systems but to intervene. the')\n",
      "(112, 14, 'literature on this is also extensive [ qmzh19, hzj+19], so we offer only a few brief comments\\non future directions speciﬁc to large language models. ... (194 characters truncated) ... nges of bias mitigation for these models. there\\nis room for more research that engages with the literature outside nlp, better articulates normative')\n",
      "(113, 14, 'statements about harm, and engages with the lived experience of communities affected by nlp\\nsystems [bbdiw20]. thus, mitigation work should not be a ... (123 characters truncated) ... 9] but in a\\nholistic manner.\\n7.3 energy usage\\npractical large-scale pre-training requires large amounts of computation, which is energy-intensive:')\n",
      "(114, 14, 'training the gpt-3 175b consumed several thousand petaﬂop/s-days of compute during pre-training,\\ncompared to tens of petaﬂop/s-days for a 1.5b param ... (184 characters truncated) ... other lens through which to view the efﬁciency of\\nlarge models - we should consider not only the resources that go into training them, but how these')\n",
      "(115, 14, 'resources are amortized over the lifetime of a model, which will subsequently be used for a variety of\\npurposes and ﬁne-tuned for speciﬁc tasks. tho ... (197 characters truncated) ... odel can cost on the order of 0.4 kw-hr, or only a\\nfew cents in energy costs. additionally, techniques like model distillation [lhcg19a] can further')\n",
      "(116, 14, 'bring down the cost of such models, letting us adopt a paradigm of training single, large-scale models,\\nthen creating more efﬁcient versions of them ... (74 characters truncated) ...  further increase the efﬁciency of such models over time, similar to trends observed\\nin image recognition and neural machine translation [hb20].\\n14')\n",
      "(117, 15, '7.4 news generation\\nwe test gpt-3’s ability to generate synthetic “news articles” by prompting the model with a context\\nof three previous news arti ... (132 characters truncated) ... ility to distinguish gpt-3-generated articles from\\nreal ones. similar work has been carried out by kreps et al. [kmb20] and zellers et al. [zhr+19].')\n",
      "(118, 15, 'generative language models are trained to match the distribution of content generated by humans, so\\nthe (in)ability of humans to distinguish the two ... (196 characters truncated) ... com (mean length: 215 words). we then generated\\ncompletions of these titles and subtitles from for language models ranging in size from 125m to 175b')\n",
      "(119, 15, '(gpt-3) parameters (mean length: 200 words). for each model, we presented around 80 us-based\\nparticipants with a quiz consisting of these real title ... (171 characters truncated) ... ly written by a human”, “more likely written by a human”, “i don’t know”,\\n“more likely written by a machine”, or “very likely written by a machine”.')\n",
      "(120, 15, 'the articles we selected were not in the models’ training data and the model outputs were formatted\\nand selected programmatically to prevent human c ... (202 characters truncated) ... owever, we also ran an experiment to control for\\nparticipant effort and attention that followed the same format but involved intentionally bad model')\n",
      "(121, 15, 'generated articles. this was done by generating articles from a “control model”: a 160m parameter\\nmodel with no context and increased output randomn ... (155 characters truncated) ... ere model generated was∼ 86% where 50% is chance\\nlevel performance. by contrast, mean human accuracy at detecting articles that were produced by the')\n",
      "(122, 15, '175b parameter model was barely above chance at∼ 52% (see table 7.3).6 human abilities to detect\\nmodel generated text appear to decrease as model si ... (206 characters truncated) ... h output as model size increases (see the\\nappendix).\\nexamples of synthetic articles from gpt-3 are given in figures 7.4 and 7.5. 8 much of the text')\n",
      "(123, 15, 'is—as indicated by the evaluations—difﬁcult for humans to distinguish from authentic human content.\\nfactual inaccuracies can be an indicator that an ... (162 characters truncated) ... nwritten. other indicators include repetition, non sequiturs, and unusual phrasings, though these are\\noften subtle enough that they are not noticed.')\n",
      "(124, 15, 'often subtle enough that they are not noticed.\\nrelated work on language model detection by ippolito et al. [ idcbe19] indicates that automatic\\ndisc ... (168 characters truncated) ... els may be a promising\\narea of future research.\\nippolito et al. [idcbe19] also note that human accuracy at detecting model generated text increases')\n",
      "(125, 15, 'as humans observe more tokens. to do a preliminary investigation of how good humans are at\\ndetecting longer news articles generated by gpt-3 175b, w ... (187 characters truncated) ... longer than our initial experiments). following the\\n4this task is also relevant to the potential misuse of language models discussed in section 7.1.')\n",
      "(126, 15, '5we wanted to identify how good an average person on the internet is at detecting language model outputs,\\nso we focused on participants drawn from t ... (153 characters truncated) ...  participant\\naccuracies of each model and the control model and report the normalized difference in the means (as the\\nt-statistic) and the p-value.')\n",
      "(127, 15, 't-statistic) and the p-value.\\n7if a model consistently produces texts that are more impressive than human articles, it is possible that human\\nperfo ... (25 characters truncated) ...  drop below 50%. indeed, many individual participants scored below 50% on\\nthis task.\\n8additional non-news samples can be found in the appendix.\\n15')\n",
      "(128, 16, 'mean accuracy\\n95% conﬁdence\\ninterval (low, hi)\\nt compared to\\ncontrol (p-value)\\n“i don’t know”\\nassignments\\ncontrol (deliberately bad model) 86% ... (209 characters truncated) ... 5% 10.4 (5 e-19) 7.1%\\ngpt-3 6.7b 60% 56%–63% 11.2 (3 e-21) 6.2%\\ngpt-3 13b 55% 52%–58% 15.3 (1 e-32) 7.1%\\ngpt-3 175b 52% 49%–54% 16.9 (1 e-34) 7.8%')\n",
      "(129, 16, 'gpt-3 175b 52% 49%–54% 16.9 (1 e-34) 7.8%\\ntable 7.3: human accuracy in identifying whether short (∼200 word) news articles are model\\ngenerated. we  ... (126 characters truncated) ... rol model to 52% on gpt-3 175b. this table compares\\nmean accuracy between ﬁve different models, and shows the results of a two-sample t-test for the')\n",
      "(130, 16, 'difference in mean accuracy between each model and the control model (an unconditional gpt-3\\nsmall model with increased output randomness).\\nmean ac ... (130 characters truncated) ... 7%\\ngpt-3 175b 52% 48%–57% 12.7 (3.2 e-23) 10.6%\\ntable 7.4: people’s ability to identify whether∼ 500 word articles are model generated (as measured')\n",
      "(131, 16, 'by the ratio of correct assignments to non-neutral assignments) was 88% on the control model and\\n52% on gpt-3 175b. this table shows the results of  ... (196 characters truncated) ...  two experiments, each on around 80 us-based participants, to compare\\nhuman abilities to detect the articles generated by gpt-3 and a control model.')\n",
      "(132, 16, 'we found that mean human accuracy at detecting the intentionally bad longer articles from the control\\nmodel was∼ 88%, while mean human accuracy at d ... (169 characters truncated) ... ound 500 words long, gpt-3 continues to produce articles that humans ﬁnd difﬁcult\\nto distinguish from human written news articles.\\nacknowledgements')\n",
      "(133, 16, 'acknowledgements\\nthe authors would like to thank ryan lowe for giving detailed feedback on drafts of the paper. thanks\\nto jakub pachocki and szymon ... (214 characters truncated) ... roject, irene solaiman for discussions about ways to\\napproach and evaluate bias, harrison edwards and yura burda for discussions and experimentation')\n",
      "(134, 16, 'with in-context learning, geoffrey irving and paul christiano for early discussions of language model\\nscaling, long ouyang for advising on the desig ... (205 characters truncated) ...  the training of the model, and to those who were\\ninvolved in indexing or upvoting the content (in the case of webtext). additionally, we would like')\n",
      "(135, 16, 'to thank the entire openai infrastructure and supercomputing teams for making it possible to train\\nmodels at this scale.\\n16')\n",
      "(136, 17, 'figure 7.3: people’s ability to identify whether news articles are model-generated (measured by the\\nratio of correct assignments to non-neutral assi ... (211 characters truncated) ...  the top, and the random chance (50%)\\nis indicated with the dashed line at the bottom. line of best ﬁt is a power law with 95% conﬁdence\\nintervals.')\n",
      "(137, 17, 'intervals.\\ncontributions\\ntom brown, ben mann, prafulla dhariwal, dario amodei, nick ryder, daniel m ziegler, and\\njeffrey wuimplemented the large-s ... (204 characters truncated) ... eduplicated, and conducted overlap analysis on\\nthe training data.\\nmelanie subbiah, ben mann, dario amodei, jared kaplan, sam mccandlish, tom brown,')\n",
      "(138, 17, 'tom henighan, and girish sastry implemented the downstream tasks and the software framework\\nfor supporting them, including creation of synthetic tas ... (219 characters truncated) ... ben mann implemented sampling without replacement during training.\\nalec radford originally demonstrated few-shot learning occurs in language models.')\n",
      "(139, 17, 'jared kaplan and sam mccandlish showed that larger models learn more quickly in-context, and\\nsystematically studied in-context learning curves, task ... (180 characters truncated) ... on child and mark chen developed an early version of our model-parallel strategy.\\nrewon child and scott gray contributed the sparse transformer.\\n17')\n",
      "(140, 18, 'title: united methodists agree to historic split\\nsubtitle: those who oppose gay marriage will form their own\\ndenomination\\narticle: after two days  ... (201 characters truncated) ... vative,\" according to\\nthe washington post. the majority of delegates attending the\\nchurch\\'s annual general conference in may voted to strengthen a')\n",
      "(141, 18, 'church\\'s annual general conference in may voted to strengthen a\\nban on the ordination of lgbtq clergy and to write new rules that\\nwill \"discipline ... (197 characters truncated) ... hodist denomination.\\nthe post notes that the denomination, which claims 12.5 million\\nmembers, was in the early 20th century the \"largest protestant')\n",
      "(142, 18, 'members, was in the early 20th century the \"largest protestant\\ndenomination in the u.s.,\" but that it has been shrinking in\\nrecent decades. the new ... (207 characters truncated) ... posed split \"comes at a\\ncritical time for the church, which has been losing members\\nfor years,\" which has been \"pushed toward the brink of a schism')\n",
      "(143, 18, 'for years,\" which has been \"pushed toward the brink of a schism\\nover the role of lgbtq people in the church.\" gay marriage is\\nnot the only issue th ... (206 characters truncated) ... acific regional conference\\nvoting to allow them.\\nfigure 7.4: the gpt-3 generated news article that humans had the greatest difﬁculty distinguishing')\n",
      "(144, 18, 'from a human written article (accuracy: 12%).\\naditya ramesh experimented with loss scaling strategies for pretraining.\\nmelanie subbiah and arvind n ... (183 characters truncated) ... \\nsandhini agarwal conducted the fairness and representation analysis.\\ngirish sastry and amanda askell conducted the human evaluations of the model.')\n",
      "(145, 18, 'ariel herbert-voss conducted the threat analysis of malicious use.\\ngretchen krueger edited and red-teamed the policy sections of the paper.\\nbenjami ... (174 characters truncated) ...  developed fast gpu kernels used during training.\\njack clark led the analysis of ethical impacts — fairness and representation, human assessments of')\n",
      "(146, 18, 'the model, and broader impacts analysis, and advised gretchen, amanda, girish, sandhini, and ariel\\non their work.\\ndario amodei, alec radford, tom brown, sam mccandlish, nick ryder, jared kaplan, sand-\\nhini agarwal, amanda askell, girish sastry, and jack clark wrote the paper.\\n18')\n",
      "(147, 19, 'title: star’s tux promise draws megyn kelly’s sarcasm\\nsubtitle: joaquin phoenix pledged to not change for each awards\\nevent\\narticle: a year ago, j ... (213 characters truncated) ... ge myself.\" it was a\\npromise to not change to fit into the hollywood mold: \"i think\\nthat\\'s a really special thing, to not change yourself. i think')\n",
      "(148, 19, 'that\\'s a really special thing, to not change yourself. i think\\nit\\'s a really special thing to say, `this is what\\'s inside of\\nme, i\\'m proud of i ... (203 characters truncated) ... ing a tux no matter what.\\nmegyn kelly was not impressed, and she let him have it on the\\ntonight show. \"you know, i feel like, i feel like you could')\n",
      "(149, 19, 'tonight show. \"you know, i feel like, i feel like you could\\nhave worn the tux,\" she says. \"but you\\'re saying you\\'re a\\nshape-shifter. i don\\'t kno ... (215 characters truncated) ... thing.\\' and then\\ni thought, `i don\\'t want to wear a tuxedo to this thing.\\'\" kelly\\ngoes on to encourage him to change his mind again, but phoenix')\n",
      "(150, 19, 'goes on to encourage him to change his mind again, but phoenix\\nsays it\\'s too late: \"i\\'m committed to wearing this.\"\\nfigure 7.5: the gpt-3 generat ... (171 characters truncated) ... tom henighan and jared kaplan\\non their work.\\nalec radford advised the project from an nlp perspective, suggested tasks, put the results in context,')\n",
      "(151, 19, 'and demonstrated the beneﬁt of weight decay for training.\\nilya sutskever was an early advocate for scaling large generative likelihood models, and advised\\npranav, prafulla, rewon, alec, and aditya on their work.\\ndario amodei designed and led the research.\\n19')\n",
      "(152, 20, 'references\\n[adg+16] marcin andrychowicz, misha denil, sergio gomez, matthew w hoffman, david\\npfau, tom schaul, brendan shillingford, and nando de f ... (162 characters truncated) ... aroni, melvin johnson, and orhan firat. massively multilingual neural\\nmachine translation. in proceedings of the 2019 conference of the north ameri-')\n",
      "(153, 20, 'can chapter of the association for computational linguistics: human language\\ntechnologies, volume 1 (long and short papers), 2019.\\n[bbdiw20] su lin ... (204 characters truncated) ... drea esuli, and fabrizio sebastiani. sentiwordnet 3.0: an en-\\nhanced lexical resource for sentiment analysis and opinion mining. in lrec, volume 10,')\n",
      "(154, 20, 'pages 2200–2204, 2010.\\n[bht+20] yonatan bisk, ari holtzman, jesse thomason, jacob andreas, yoshua bengio, joyce\\nchai, mirella lapata, angeliki laza ... (215 characters truncated) ... dients through stochastic neurons for conditional computation. arxiv, 2013.\\n[car97] rich caruana. multitask learning. machine learning, 28(1), 1997.')\n",
      "(155, 20, '[cce+18] peter clark, isaac cowhey, oren etzioni, tushar khot, ashish sabharwal, carissa\\nschoenick, and oyvind tafjord. think you have solved questi ... (147 characters truncated) ... er. generating long sequences\\nwith sparse transformers, 2019.\\n[cly+19] yen-chun chen, linjie li, licheng yu, ahmed el kholy, faisal ahmed, zhe gan,')\n",
      "(156, 20, 'yu cheng, and jingjing liu. uniter: learning universal image-text representations.\\narxiv preprint arxiv:1909.11740, 2019.\\n[cra17] kate crawford. th ... (182 characters truncated) ... language understanding. arxiv preprint\\narxiv:1810.04805, 2018.\\n[dgv+18] mostafa dehghani, stephan gouws, oriol vinyals, jakob uszkoreit, and lukasz')\n",
      "(157, 20, 'kaiser. universal transformers. arxiv, 2018.\\n[dhkh14] nadir durrani, barry haddow, philipp koehn, and kenneth heaﬁeld. edinburgh’s\\nphrase-based mac ... (205 characters truncated) ... inadvances in\\nneural information processing systems, 2015.\\n[dsc+16] yan duan, john schulman, xi chen, peter l. bartlett, ilya sutskever, and pieter')\n",
      "(158, 20, 'abbeel. rl 2: fast reinforcement learning via slow reinforcement learning. arxiv,\\nabs/1611.02779, 2016.\\n[dwd+19] dheeru dua, yizhong wang, pradeep  ... (40 characters truncated) ...  and\\nmatt gardner. drop: a reading comprehension benchmark requiring discrete reasoning\\nover paragraphs. arxiv preprint arxiv:1903.00161, 2019.\\n20')\n",
      "(159, 21, '[dyy+19] zihang dai, zhilin yang, yiming yang, jaime g. carbonell, quoc v . le, and ruslan\\nsalakhutdinov. transformer-xl: attentive language models  ... (199 characters truncated) ... l17] chelsea finn, pieter abbeel, and sergey levine. model-agnostic meta-learning for\\nfast adaptation of deep networks. arxiv, abs/1703.03400, 2017.')\n",
      "(160, 21, 'fast adaptation of deep networks. arxiv, abs/1703.03400, 2017.\\n[gg19] hila gonen and yoav goldberg. lipstick on a pig: debiasing methods cover up\\ns ... (216 characters truncated) ... gururangan, swabha swayamdipta, omer levy, roy schwartz, samuel r\\nbowman, and noah a smith. annotation artifacts in natural language inference data.')\n",
      "(161, 21, 'arxiv preprint arxiv:1803.02324, 2018.\\n[gsr19] sebastian gehrmann, hendrik strobelt, and alexander m. rush. gltr: statistical\\ndetection and visuali ... (158 characters truncated) ... r low-resource neural machine translation. arxiv preprint arxiv:1808.08437, 2018.\\n[hb20] daniel hernandez and tom brown. ai and efﬁciency, may 2020.')\n",
      "(162, 21, '[hna+17] joel hestness, sharan narang, newsha ardalani, gregory diamos, heewoo jun,\\nhassan kianinejad, md. mostofa ali patwary, yang yang, and yanqi ... (179 characters truncated) ... in a neural\\nnetwork. arxiv preprint arxiv:1503.02531, 2015.\\n[hyc01] sepp hochreiter, a steven younger, and peter r conwell. learning to learn using')\n",
      "(163, 21, 'gradient descent. in international conference on artiﬁcial neural networks, pages\\n87–94. springer, 2001.\\n[hzj+19] po-sen huang, huan zhang, ray jia ... (154 characters truncated) ... actual evaluation. arxiv preprint arxiv:1911.03064,\\n2019.\\n[idcbe19] daphne ippolito, daniel duckworth, chris callison-burch, and douglas eck. auto-')\n",
      "(164, 21, 'matic detection of generated text is easiest when humans are fooled. arxiv preprint\\narxiv:1911.00650, 2019.\\n[jcwz17] mandar joshi, eunsol choi, dan ... (192 characters truncated) ... gamma lab nyc. numeric transformer - albert, march 2020.\\n[jys+19] xiaoqi jiao, yichun yin, lifeng shang, xin jiang, xiao chen, linlin li, fang wang,')\n",
      "(165, 21, 'and qun liu. tinybert: distilling bert for natural language understanding. arxiv\\npreprint arxiv:1909.10351, 2019.\\n[jzc+19] ying ju, fubang zhao, sh ... (218 characters truncated) ... tafjord, peter clark, and\\nhannaneh hajishirzi. uniﬁedqa: crossing format boundaries with a single qa system.\\narxiv preprint arxiv:2005.00700, 2020.')\n",
      "(166, 21, 'arxiv preprint arxiv:2005.00700, 2020.\\n21')\n",
      "(167, 22, '[kmb20] sarah e. kreps, miles mccain, and miles brundage. all the news that’s ﬁt to fabricate:\\nai-generated text as a tool of media misinformation,  ... (145 characters truncated) ...  dario amodei. scaling laws\\nfor neural language models, 2020.\\n[kpr+19] tom kwiatkowski, jennimaria palomaki, olivia redﬁeld, michael collins, ankur')\n",
      "(168, 22, 'parikh, chris alberti, danielle epstein, illia polosukhin, matthew kelcey, jacob\\ndevlin, kenton lee, kristina n. toutanova, llion jones, ming-wei ch ... (218 characters truncated) ... exander m. rush. sequence-level knowledge distillation. arxiv,\\n2016.\\n[lb02] edward loper and steven bird. nltk: the natural language toolkit, 2002.')\n",
      "(169, 22, '[lc19] guillaume lample and alexis conneau. cross-lingual language model pretraining.\\narxiv preprint arxiv:1901.07291, 2019.\\n[lcg+19] zhenzhong lan ... (215 characters truncated) ... ong liu, hao cheng, pengcheng he, weizhu chen, yu wang, hoifung poon,\\nand jianfeng gao. adversarial training for large neural language models. arxiv')\n",
      "(170, 22, 'preprint arxiv:2004.08994, 2020.\\n[lcr19] peter j. liu, yu-an chung, and jie ren. summae: zero-shot abstractive text sum-\\nmarization using length-ag ... (220 characters truncated) ... atao gu, naman goyal, xian li, sergey edunov, marjan ghazvininejad,\\nmike lewis, and luke zettlemoyer. multilingual denoising pre-training for neural')\n",
      "(171, 22, 'machine translation. arxiv preprint arxiv:2001.08210, 2020.\\n[lgh+15] xiaodong liu, jianfeng gao, xiaodong he, li deng, kevin duh, and ye-yi wang.\\nr ... (214 characters truncated) ... utational linguistics: human language\\ntechnologies, 2015.\\n[lhcg19a] xiaodong liu, pengcheng he, weizhu chen, and jianfeng gao. improving multi-task')\n",
      "(172, 22, 'deep neural networks via knowledge distillation for natural language understanding.\\narxiv preprint arxiv:1904.09482, 2019.\\n[lhcg19b] xiaodong liu,  ... (172 characters truncated) ... s, yinhan liu, naman goyal, marjan ghazvininejad, abdelrahman mo-\\nhamed, omer levy, ves stoyanov, and luke zettlemoyer. bart: denoising sequence-to-')\n",
      "(173, 22, 'sequence pre-training for natural language generation, translation, and comprehension.\\narxiv preprint arxiv:1910.13461, 2019.\\n[lm17] ke li and jite ... (184 characters truncated) ... e lewis, luke zettlemoyer, and veselin stoyanov. roberta: a robustly\\noptimized bert pretraining approach. arxiv preprint arxiv:1907.11692, 2019.\\n22')\n",
      "(174, 23, '[lpp+20] patrick lewis, ethan perez, aleksandra piktus, fabio petroni, vladimir karpukhin,\\nnaman goyal, heinrich k¨uttler, mike lewis, wen-tau yih,  ... (182 characters truncated) ... eric wallace, sheng shen, kevin lin, kurt keutzer, dan klein, and\\njoseph e. gonzalez. train large, then compress: rethinking model size for efﬁcient')\n",
      "(175, 23, 'training and inference of transformers, 2020.\\n[mch+16] nasrin mostafazadeh, nathanael chambers, xiaodong he, devi parikh, dhruv ba-\\ntra, lucy vande ... (144 characters truncated) ... rxiv:1604.01696, 2016.\\n[mkat18] sam mccandlish, jared kaplan, dario amodei, and openai dota team. an empirical\\nmodel of large-batch training, 2018.')\n",
      "(176, 23, 'model of large-batch training, 2018.\\n[mkm+94] mitchell marcus, grace kim, mary ann marcinkiewicz, robert macintyre, ann bies,\\nmark ferguson, karen  ... (153 characters truncated) ... pages 114–119. association for computational linguistics, 1994.\\n[mkxs18] bryan mccann, nitish shirish keskar, caiming xiong, and richard socher. the')\n",
      "(177, 23, 'natural language decathlon: multitask learning as question answering. arxiv preprint\\narxiv:1806.08730, 2018.\\n[mpl19] r thomas mccoy, ellie pavlick, ... (202 characters truncated) ... ivar, parker barnes, lucy vasserman,\\nben hutchinson, elena spitzer, inioluwa deborah raji, and timnit gebru. model\\ncards for model reporting, 2018.')\n",
      "(178, 23, 'cards for model reporting, 2018.\\n[nbr20] moin nadeem, anna bethke, and siva reddy. stereoset: measuring stereotypical bias\\nin pretrained language m ... (141 characters truncated) ... e arguments. arxiv preprint arxiv:1907.07355, 2019.\\n[nvnvdg19] malvina nissim, rik van noord, and rob van der goot. fair is better than sensational:')\n",
      "(179, 23, 'man is to doctor as woman is to doctor. arxiv preprint arxiv:1905.09866, 2019.\\n[or16] university of regensburg. fascha, 2016.\\n[pfb18] jason phang,  ... (193 characters truncated) ...  germ´an kruszewski, angeliki lazaridou, quan ngoc pham, raffaella\\nbernardi, sandro pezzelle, marco baroni, gemma boleda, and raquel fern´andez. the')\n",
      "(180, 23, 'lambada dataset: word prediction requiring a broad discourse context. arxiv preprint\\narxiv:1606.06031, 2016.\\n[pos18] matt post. a call for clarity  ... (192 characters truncated) ... -equalizing loss function. arxiv preprint\\narxiv:1905.12801, 2019.\\n[rcm19] siva reddy, danqi chen, and christopher d manning. coqa: a conversational')\n",
      "(181, 23, 'question answering challenge. transactions of the association for computational\\nlinguistics, 7:249–266, 2019.\\n23')\n",
      "(182, 24, '[rcp+17] scott reed, yutian chen, thomas paine, a¨aron van den oord, sm eslami, danilo\\nrezende, oriol vinyals, and nando de freitas. few-shot autore ... (165 characters truncated) ... as a model for few-shot learning.\\niclr 2017 (oral), 2016.\\n[rll+19] qiu ran, yankai lin, peng li, jie zhou, and zhiyuan liu. numnet: machine reading')\n",
      "(183, 24, 'comprehension with numerical reasoning. in proceedings of emnlp, 2019.\\n[rnlvd18] rachel rudinger, jason naradowsky, brian leonard, and benjamin van  ... (188 characters truncated) ... onathan s. rosenfeld, amir rosenfeld, yonatan belinkov, and nir shavit. a con-\\nstructive prediction of the generalization error across scales, 2019.')\n",
      "(184, 24, '[rrs20] adam roberts, colin raffel, and noam shazeer. how much knowledge can you pack\\ninto the parameters of a language model? arxiv preprint arxiv: ... (178 characters truncated) ...  transfer learning\\nwith a uniﬁed text-to-text transformer, 2019.\\n[rwc+19] alec radford, jeffrey wu, rewon child, david luan, dario amodei, and ilya')\n",
      "(185, 24, 'sutskever. language models are unsupervised multitask learners, 2019.\\n[sbc+19] irene solaiman, miles brundage, jack clark, amanda askell, ariel herb ... (158 characters truncated) ...  release strategies\\nand the social impacts of language models, 2019.\\n[scnp19] emily sheng, kai-wei chang, premkumar natarajan, and nanyun peng. the')\n",
      "(186, 24, 'woman worked as a babysitter: on biases in language generation. arxiv preprint\\narxiv:1909.01326, 2019.\\n[sdcw19] victor sanh, lysandre debut, julien ... (191 characters truncated) ... . smith, and oren etzioni. green ai. corr,\\nabs/1907.10597, 2019.\\n[shb15] rico sennrich, barry haddow, and alexandra birch. improving neural machine')\n",
      "(187, 24, 'translation models with monolingual data. arxiv preprint arxiv:1511.06709, 2015.\\n[smm+17] noam shazeer, azalia mirhoseini, krzysztof maziarz, andy d ... (192 characters truncated) ... ybi, mostofa patwary, raul puri, patrick legresley, jared casper,\\nand bryan catanzaro. megatron-lm: training multi-billion parameter language models')\n",
      "(188, 24, 'using model parallelism, 2019.\\n[ss20] timo schick and hinrich sch ¨utze. exploiting cloze questions for few-shot text\\nclassiﬁcation and natural lan ... (153 characters truncated) ... equence pre-training for language generation. arxiv preprint arxiv:1905.02450,\\n2019.\\n[tur20] project turing. microsoft research blog, feb 2020.\\n24')\n",
      "(189, 25, '[vbl+16] oriol vinyals, charles blundell, timothy lillicrap, daan wierstra, et al. matching\\nnetworks for one shot learning. in advances in neural in ... (218 characters truncated) ... in advances in\\nneural information processing systems, 2017.\\n[wpn+19] alex wang, yada pruksachatkun, nikita nangia, amanpreet singh, julian michael,')\n",
      "(190, 25, 'felix hill, omer levy, and samuel bowman. superglue: a stickier benchmark for\\ngeneral-purpose language understanding systems. in advances in neural  ... (196 characters truncated) ... 8.\\n[xdh+19] qizhe xie, zihang dai, eduard hovy, minh-thang luong, and quoc v . le. unsuper-\\nvised data augmentation for consistency training, 2019.')\n",
      "(191, 25, 'vised data augmentation for consistency training, 2019.\\n[ydy+19] zhilin yang, zihang dai, yiming yang, jaime carbonell, ruslan salakhutdinov, and\\nq ... (153 characters truncated) ... ari holtzman, yonatan bisk, ali farhadi, and yejin choi. hellaswag:\\ncan a machine really ﬁnish your sentence? arxiv preprint arxiv:1905.07830, 2019.')\n",
      "(192, 25, '[zhr+19] rowan zellers, ari holtzman, hannah rashkin, yonatan bisk, ali farhadi, franziska\\nroesner, and yejin choi. defending against neural fake ne ... (95 characters truncated) ... y wu, tom b. brown, alec radford, dario\\namodei, paul christiano, and geoffrey irving. fine-tuning language models from\\nhuman preferences, 2019.\\n25')\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_embeddings_from_chunks():\n",
    "    metadata = [{'page_number': pn} for pn in df_chunks['page_number']]\n",
    "\n",
    "    embeddings = OllamaEmbeddings(\n",
    "        model=\"llama3.2\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    vector_store = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=\"documents\",\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "    vector_store.add_texts(texts, metadatas=metadata)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "vector_size = len(embeddings.embed_query(\"test\"))\n",
    "\n",
    "if not client.collection_exists(\"documents\"):\n",
    "    client.create_collection(\n",
    "        collection_name=\"documents\",\n",
    "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "    )"
   ],
   "id": "694f3b26253ff1e3"
  },
  {
   "cell_type": "code",
   "id": "2e72d21d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T15:13:01.166724Z",
     "start_time": "2026-01-17T15:13:01.158829Z"
    }
   },
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "# Construct a tool for retrieving context\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(llm, tools, system_prompt=prompt)"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "ce43a0e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T15:13:12.288991Z",
     "start_time": "2026-01-17T15:13:01.170016Z"
    }
   },
   "source": [
    "user_query = userinput.get_user_input(\"Stelle deine Frage!\", default=\"Worum geht es in dem Dokument?\")\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "Welche Informationen enthält das Dokument?\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (c6048f02-67fa-495f-9f09-26a85c808bfa)\n",
      " Call ID: c6048f02-67fa-495f-9f09-26a85c808bfa\n",
      "  Args:\n",
      "    query: {'type': 'string'}\n",
      "    object: <nil>\n",
      "    key: <nil>\n",
      "=================================\u001B[1m Tool Message \u001B[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Error invoking tool 'retrieve_context' with kwargs {'query': {'type': 'string'}, 'object': '<nil>', 'key': '<nil>'} with error:\n",
      " query: Input should be a valid string\n",
      " Please fix the error and try again.\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Es tut mir leid, aber ich habe nicht genug Informationen über das Dokument, um Ihnen eine genaue Antwort zu geben. Könnten Sie bitte mehr Kontext oder Details zum Dokument bereitstellen? Ich werde mein Bestes tun, um Ihnen mit der verfügbaren Information zu helfen.\n",
      "\n",
      "Wenn Sie jedoch nach allgemeinen Informationen über ein Dokument suchen, kann ich Ihnen sagen, dass das Dokument möglicherweise Informationen über eine bestimmte Thematik, ein Ereignis oder einen Prozess enthält. Es könnte auch eine Anleitung, ein Handbuch oder eine andere Art von Dokument sein.\n",
      "\n",
      "Könnten Sie mir bitte mehr über das Dokument erzählen? Was ist es und was soll ich wissen? Ich werde mein Bestes tun, um Ihnen zu helfen.\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
