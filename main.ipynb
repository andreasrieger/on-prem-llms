{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T06:20:42.590743Z",
     "start_time": "2025-11-15T06:20:42.059154Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install qdrant-client ollama sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T06:20:44.284999Z",
     "start_time": "2025-11-15T06:20:44.262966Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "from modules import qdrant\n",
    "from modules.sqlite import write_df_to_sqlite\n",
    "from modules.archiver import generate_knowledge\n",
    "from modules.vectorizer import vectorize_dataframe\n",
    "\n",
    "file_name = '20251115-mfi-products.csv'\n",
    "delimiter = ';'\n",
    "collection_name = 'test_collection'\n",
    "\n",
    "# Load data, process it, and store in SQLite and Qdrant\n",
    "def load_data_and_process():\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(os.path.join(os.getcwd(),f\"input/{file_name}\"), delimiter=delimiter)\n",
    "    return generate_knowledge(df)\n",
    "\"\"\"\n",
    "    # store the dataframe in sqlite\n",
    "    res = write_df_to_sqlite(df)\n",
    "    if not res:\n",
    "        print(\"Error storing DataFrame in SQLite.\")\n",
    "\n",
    "    # Vectorize the DataFrame\n",
    "    df_vec = vectorize_dataframe(df)\n",
    "\n",
    "    # Store the vectors in Qdrant\n",
    "    res = qdrant.store_vectors_in_qdrant(df_vec, collection_name=collection_name)\n",
    "    if not res:\n",
    "        print(\"Error storing vectors in Qdrant.\")\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "knowledge_json = load_data_and_process()\n",
    "print(\"Completed initialization and data processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T06:41:09.142954Z",
     "start_time": "2025-11-15T06:40:18.579808Z"
    }
   },
   "outputs": [],
   "source": [
    "import ollama\n",
    "from modules import input, qdrant, vectorizer\n",
    "\n",
    "models = ['mistral','stablelm2','avr/sfr-embedding-mistral']\n",
    "model = models[0] # Choose the model to use\n",
    "\n",
    "\n",
    "# Generate answer based on prompt\n",
    "def generate_answer(prompt, model='mistral'):\n",
    "    return ollama.generate(\n",
    "        model=model,\n",
    "        prompt=prompt\n",
    "    )\n",
    "\n",
    "\n",
    "# Get user input\n",
    "def get_user_input():\n",
    "    #\n",
    "    qry = input.get_user_input(\"Enter search term\")\n",
    "    if input.validate_search_term(qry):\n",
    "        # Proceed with search\n",
    "        pass\n",
    "    return qry\n",
    "\n",
    "\n",
    "# Generate SQL query based on user prompt\n",
    "def agent_sql_query(prompt, knowledge, model='mistral'):\n",
    "\n",
    "    # Create the agent prompt\n",
    "    agent_info = f\"You are an AI language model assistant. Your task is to find matching products to the {prompt} using only results from a product database with this schema: {knowledge}. Create an SQL query that returns the product name aliases and descriptions of the matching products.\"\n",
    "    res = generate_answer(agent_info, model=model)\n",
    "    return res['response']\n",
    "\n",
    "\n",
    "# Perform vector search based on user prompt\n",
    "def agent_vector_search(prompt, knowledge, collection_name, model='mistral'):\n",
    "\n",
    "    agent_info = f\"You are an AI language model assistant. Your task is to find matching products to the {prompt} using only results from a product database that is described by {knowledge}. Create an embedding vector for the search term and use it to search the vector database for the most relevant products.\"\n",
    "    search_result = qdrant.perform_search_in_qdrant(\n",
    "        query = vectorizer.generate_embeddings(agent_info),\n",
    "        collection_name = collection_name,\n",
    "        limit = 1\n",
    "    )\n",
    "    res = generate_answer(f\"Based on the following search result: {search_result[0].payload}, provide a concise answer about the product.\", model=model)\n",
    "    return res['response']\n",
    "\n",
    "\n",
    "# Main execution\n",
    "search_query = get_user_input()\n",
    "answer = agent_sql_query(search_query, knowledge_json, model=model)\n",
    "# answer = agent_vector_search(search_query, knowledge_json, collection_name, model=model)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
